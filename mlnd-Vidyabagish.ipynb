{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vidyabagish\n",
    "\n",
    "## MLND - Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Background\n",
    "\n",
    "Deep learning (also known as deep structured learning or hierarchical learning) is the application of artificial neural networks (ANNs) to learning tasks that contain more than one hidden layer. Deep learning is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms.With Deep Learning, it is now possible for an algorithm to predict things, classify images (objects) with great accuracy, detect fraudulent transactions, generate image, sound and text. These are tasks that were previously not possible to achieve by an algorithm and now perform better than a human.\n",
    "\n",
    "In this project we will focus on Text Generation. Text Generation is part of Natural Language Processing and can be used totranscribe speech to text, performmachine translation, generate handwritten text, image captioning, generate new blog posts or news headlines.\n",
    "\n",
    "![Basic RNN -> Unrolled RNN](images/basic_unrolled_RNN.png)\n",
    "\n",
    "In order to generate text, we will look at a class of Neural Network where connections between units form a directed cycle, called Recurrent Neural Network (RNNs). RNNs use an internal memory to process sequences of elements and is able to learn from the syntactic structure of text. Our model will be able to generate text based on the text we train it with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Ramchandra Vidyabagish, was an Indian lexicographer and Sanskrit scholar. He is known for his Bangabhashabhidhan, the first monolingual Bengali dictionary, published in 1817.\n",
    "\n",
    "Unfortunately, Vidyabagish passed away 100 years ago and he will not be publishing new novels. But, wouldn’t it be great if we could generate some text inspired on Jyotish Sangrahasar and other novels he published?\n",
    "\n",
    "To solve our problem, we can use text from novels written by Vidyabagish in combination with the incredible power of Deep Learning, in particular RNNs, to generate text. Our deep learning model will be trained on existing Vidyabagish works and will output new text, based on the internal representation of the text it was trained on, in the Neural Network.  \n",
    "\n",
    "![LSTM Cell](images/lstm_cell.png)\n",
    "\n",
    "LSTM Cell\n",
    "\n",
    "For our model to learn, we will use a special type of RNN called LSTMs (Long Short Term Memory), capable of learning long-term dependencies. LSTM can use its memory to generate complex, [realistic sequences](https://arxiv.org/pdf/1308.0850.pdf) containing long-range structure, just like the sentences that we want to generate. It will be able to remember information for a period of time, which will help at generating text of better quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "The purpose of this project, is to create a model that will be able to generate text inspired in novels written by Vidyabagish.\n",
    "\n",
    "The performance of our model will be measure by:\n",
    "- [Perplexity](https://en.wikipedia.org/wiki/Perplexity) is a commonly used evaluation metric when generating text. Perplexity tells us how many words is the model considering as an output, having a perplexity of 3 means that the model could choose 3 words at an equally likely probability, we want our perplexity be as low as possible since lower choice corresponds to a higher likelihood of choosing the actually correct one.\n",
    "\n",
    "    The typical measure reported in the papers is average per-word perplexity (often just called perplexity), which is equal to\n",
    "\n",
    "    $$e^{-\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}} = e^{\\text{loss}}$$\n",
    "\n",
    "    Our goal is to achieving a **perplexity of less than 3.** Which is lower than the perplexity achieved by [similar models](https://web.stanford.edu/class/cs224n/reports/2737434.pdf) used for text generation. \n",
    "\n",
    "\n",
    "- Grammatically our model is able to:\n",
    "    - Open, close quotations\n",
    "    - Sentence length is similar to the median sentence length of the dataset. We will use the median as there is a large number of empty sentences (between paragraphs, separating chapters, after a title), which can skew our data. See histogram below.\n",
    "    - Create paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we will use the text from his novel Jyotish Sangrahasar and Bachaspati Mishrer Vivadachintamanih. All the novels are no longer protected under copyright and thanks to David Hare for providing the pdf file of these book.\n",
    "\n",
    "Even though Vidyabagish native language was Bengali, the text used to train our model will be in English. This is to make it easier for the reader to understand the input and output of our model.\n",
    "\n",
    "Our Dataset is small as it is composed of only 2 files - Jyotish Sangrahasar and Bachaspati Mishrer Vivadachintamanih with a total size of 3.4 MB. Bigger datasets work better when training an RNN but for our case that is very specific it will be enough. Some additional information of the contents of the files below:\n",
    "\n",
    "| Name | Size | Pages | Lines | Words | Unique Words |\n",
    "|:--- |:---:|:---:| ---:| ----:| ----:| ----:| ----:| -----:|\n",
    "| Jyotish_Sangrahasar.txt | 2.3 MB| 690 | 40,008 | 429,256 | 42,154 | \n",
    "|Bachaspati_Mishrer_Vivadachintamanih.txt | 1.1 MB | 303 | 17,572 | 189,037 |  |\n",
    "\n",
    "* Note: Values in the table above will change after preprocessing.\n",
    "\n",
    "There is some manual preprocessing that we will need to do as the text retrieved from Gutenberg Project contains additional content that is not necessary to train the model, for example:\n",
    "\n",
    "* Preface\n",
    "* Translator’s Preface\n",
    "* About the author\n",
    "* Index\n",
    "* Dedications\n",
    "* Footnotes included in Exemplary Novels\n",
    "\n",
    "**Note:** The files included in the dataset folder no longer contain the additional content mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Lets start by loading our Data and exploring it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "filenames = [\"/home/kabya/Documents/Vidyabagish-text-generation/dataset/Jyotish_Sangrahasar.txt\", \"/home/kabya/Documents/Vidyabagish-text-generation/dataset/Bachaspati_Mishrer_Vivadachintamanih.txt\"]\n",
    "\n",
    "text = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring our Datasets\n",
    "\n",
    "Lets extract text from our Datasets to get familiar with the data that we will be processing.\n",
    "\n",
    "We can see that our sentences are formed of 13 / 14 words. Paragraphs contain 5 or more sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_contents = \"\"\n",
    "\n",
    "with open(filenames[0], \"r\") as f:\n",
    "    file_contents += f.read()\n",
    "\n",
    "text += file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "himself up to reading\n",
      "books of chivalry with such ardour and avidity that he almost entirely\n",
      "neglected the pursuit of his field-sports, and even the management of his\n",
      "property; and to such a pitch did his eagerness and infatuation go that\n",
      "he sold many an acre of tillageland to buy books of chivalry to read, and\n",
      "brought home as many of them as he could get. But of all there were none\n",
      "he liked so well as those of the famous Feliciano de Silva's composition,\n",
      "for their lucidity of style and complicated conceits were as pearls in\n",
      "his sight, particularly when in his reading he came upon courtships and\n",
      "cartels, where he often found passages like \"the reason of the unreason\n",
      "with which my reason is afflicted so weakens my reason that with reason I\n",
      "murmur at your beauty;\" or again, \"the high heavens, that of your\n",
      "divinity divinely fortify you with the stars, render you deserving of the\n",
      "desert your greatness deserves.\" Over conceits of this sort the poor\n",
      "gentleman lost his wits, and used to lie a\n"
     ]
    }
   ],
   "source": [
    "# Sample text of Jyotish_Sangrahasar.txt\n",
    "print(file_contents[1500:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_contents = \"\"\n",
    "\n",
    "with open(filenames[1], \"r\") as f:\n",
    "    file_contents += f.read()\n",
    "    \n",
    "text += file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t all the pains they took for that\n",
      "purpose proved vain, and the wishes they had felt on the subject\n",
      "gradually diminished, as the attempt appeared more and more hopeless.\n",
      "Thus, devoted to their studies, and varying these with such amusements\n",
      "as are permitted to their age, the young men passed a life as cheerful\n",
      "as it was honourable, rarely going out at night, but when they did so,\n",
      "it was always together and well armed.\n",
      "\n",
      "One evening, however, when Don Juan was preparing to go out, Don\n",
      "Antonio expressed his desire to remain at home for a short time, to\n",
      "repeat certain orisons: but he requested Don Juan to go without him, and\n",
      "promised to follow him.\n",
      "\n",
      "\"Why should I go out to wait for you?\" said Don Juan. \"I will stay; if\n",
      "you do not go out at all to-night, it will be of very little\n",
      "consequence.\" \"By no means shall you stay,\" returned Don Antonio: \"go\n",
      "and take the air; I will be with you almost immediately, if you take the\n",
      "usual way.\"\n",
      "\n",
      "\"Well, do as you please,\" said Don Juan: \"if you come you \n"
     ]
    }
   ],
   "source": [
    "# Sample text of Bachaspati_Mishrer_Vivadachintamanih.txt\n",
    "print(file_contents[3500:4500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics of our Dataset\n",
    "\n",
    "As explained in the Metrics section, we can see that there is a large number of empty sentences in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing data to do some stats\n",
    "chapters = text.split('\\n\\n\\n\\n')\n",
    "sentence_count_chapter = [chapter.count('\\n') for chapter in chapters]\n",
    "sentences = [sentence for chapter in chapters for sentence in chapter.split('\\n')]\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Unique words: 39229\n",
      "Number of chapters: 135\n",
      "Average number of sentences in each chapters: 392.7925925925926\n",
      "Number of lines: 53162\n",
      "Median number of words in each line: 13.0\n",
      "Average number of words in each line: 10.975941461946503\n"
     ]
    }
   ],
   "source": [
    "print('Dataset Stats')\n",
    "print('Unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "print('Number of chapters: {}'.format(len(chapters)))\n",
    "print('Average number of sentences in each chapters: {}'.format(np.average(sentence_count_chapter)))\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "print('Median number of words in each line: {}'.format(np.median(word_count_sentence)))\n",
    "print('Average number of words in each line: {}'.format(np.mean(word_count_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/V0FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb65bf3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(word_count_sentence, bins=\"auto\")\n",
    "plt.title(\"Number of words per sentence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Preprocessing \n",
    "We need to prepare our data for our RNN, lets do some additional preprocessing:\n",
    "* Lookup table: We need to create [word embeddings](https://www.tensorflow.org/tutorials/word2vec#motivation_why_learn_word_embeddings) to facilitate the training of our model. \n",
    "\n",
    "* Tokenize punctuation: This is to simplify training for our neural network. Making it easy for it to distinguish between *mad* and *mad!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of dataset split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    \n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = {v:k for k, v in vocab_to_int.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"mad\" and \"mad!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lookup = {\".\": \"||period||\", \\\n",
    "         \",\": \"||comma||\", \\\n",
    "         '\"': \"||quotation_mark||\", \\\n",
    "         \";\": \"||semicolon||\", \\\n",
    "         \"!\": \"||exclamation_mark||\", \\\n",
    "         \"?\": \"||question_mark||\", \\\n",
    "         \"(\": \"||l_parenthesis||\", \\\n",
    "         \")\": \"||r_parenthesis||\", \\\n",
    "         \"--\": \"||dash||\", \\\n",
    "         \"\\n\": \"||return||\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for key, token in token_lookup.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "text = text.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "\n",
    "# Saving the preprocessed data\n",
    "pickle.dump((int_text, vocab_to_int, int_to_vocab, token_lookup), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Check Point\n",
    "The preprocessed data has been saved to disk. No need to preprocess it again, by running the cell below it will be available to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vidyabagish Neural Network\n",
    "Before getting started, lets check some requirements to run the Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "\n",
    "A GPU is suggested to train the Vidyabagish Neural Network as text generation takes a long time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network as text generation takes a long time to train in order to achieve good results.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Code\n",
    "The building blocks of the Vidyabagish Neural Network are include in Vidyabagish_nn.py. If you want to view the code run *Vidyabagishnn??* in a separate cell after importing it.\n",
    "\n",
    "Functions included in Vidyabagish_nn:\n",
    "- get_inputs: Creates the TF Placeholders for the Neural Network\n",
    "- get_init_cell: Creates our RNN cell and initialises it.\n",
    "- get_embed: Applies [embedding](https://www.tensorflow.org/tutorials/word2vec) to our input data.\n",
    "- build_rnn: Creates a RNN using a RNN cell\n",
    "- build_nn: Apply embedding to input data using your get_embed function. Builds RNN using cell and the build_rnn function. Finally, it applies a [fully connected layer](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected) with a linear activation.\n",
    "- get_batches: Creates a generator that returns batches of data used during training\n",
    "\n",
    "#### Loss Function\n",
    "To calculate the loss of our Neural Network, we are using [sequence_loss](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss) as our loss function.\n",
    "\n",
    "We want to minimize the average negative log probability of the target words:\n",
    "\n",
    "$$loss = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}$$\n",
    "\n",
    "The typical measure reported in the papers is average per-word perplexity (often just called perplexity), which is equal to\n",
    "\n",
    "$$e^{-\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}} = e^{\\text{loss}}$$\n",
    "\n",
    "and we will monitor its value throughout the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Vidyabagish_nn as Vidyabagishnn\n",
    "\n",
    "Vidyabagishnn.reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View the code of Vidyabagish_nn\n",
    "Vidyabagishnn??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vidyabagish Neural Network Training\n",
    "### Hyperparameters\n",
    "The following parameters are used to tune the Neural Network:\n",
    "\n",
    "- `batch_size`: The number of training examples in one pass.\n",
    "- `num_epochs`: One pass of all the training examples.\n",
    "- `rnn_layer_size`: Number of RNN layers\n",
    "- `rnn_size`: Size of the RNNs.\n",
    "- `embed_dim`: Size of the embedding.\n",
    "- `seq_length`: Number of words included in every sequence, e.g. sequence of five words. \n",
    "- `learning_rate`: How fast/slow the Neural Network will train.\n",
    "- `dropout`: Simple way to prevents an RNN from overfitting - [link](http://jmlr.org/papers/v15/srivastava14a.html).\n",
    "- `show_every_n_batches`: Number of batches the neural network should print progress.\n",
    "- `save_every_n_epochs`: Number of epochs the neural network should save progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 512\n",
    "# Number of Epochs\n",
    "num_epochs = 700\n",
    "# RNN Layers\n",
    "rnn_layer_size = 2\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "# Using 300 as it is commonly used in Google's news word vectors and the GloVe vectors\n",
    "embed_dim = 300\n",
    "# Sequence Length\n",
    "seq_length = 10\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout\n",
    "dropout = 0.6\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "# Save progress for every n number of epochs\n",
    "save_every_n_epochs = 100\n",
    "\n",
    "run_id = '0007'\n",
    "\n",
    "# Define saving directories\n",
    "save_dir = './checkpoints/save'\n",
    "logs_dir = './logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using Vidyabagish neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Layers: 2 and Size: 256, Batch Size: Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "(?, ?, 300)\n",
      "RNN Layers: 2 and Size: 256, Batch Size: 512\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # Inputs\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = Vidyabagishnn.get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    \n",
    "    # Define the RNN cell\n",
    "    cell, initial_state = Vidyabagishnn.get_init_cell(batch_size=input_data_shape[0], \n",
    "                                               rnn_layers=rnn_layer_size, \n",
    "                                               rnn_size=rnn_size,\n",
    "                                               keep_prob=dropout)\n",
    "    # Builds Neural Network\n",
    "    logits, final_state = Vidyabagishnn.build_nn(cell, input_text, vocab_size, embed_dim,\n",
    "                                         batch_size, rnn_layer_size, rnn_size, dropout)\n",
    "\n",
    "    \n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train Vidyabagish neural network on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/141   train_loss = 9.989\n",
      "Epoch   0 Batch  100/141   train_loss = 6.210\n",
      "Epoch   1 Batch   59/141   train_loss = 6.192\n",
      "Epoch   2 Batch   18/141   train_loss = 6.254\n",
      "Epoch   2 Batch  118/141   train_loss = 6.199\n",
      "Epoch   3 Batch   77/141   train_loss = 6.179\n",
      "Epoch   4 Batch   36/141   train_loss = 6.242\n",
      "Epoch   4 Batch  136/141   train_loss = 6.187\n",
      "Epoch   5 Batch   95/141   train_loss = 6.175\n",
      "Epoch   6 Batch   54/141   train_loss = 6.260\n",
      "Epoch   7 Batch   13/141   train_loss = 6.229\n",
      "Epoch   7 Batch  113/141   train_loss = 6.191\n",
      "Epoch   8 Batch   72/141   train_loss = 6.203\n",
      "Epoch   9 Batch   31/141   train_loss = 6.196\n",
      "Epoch   9 Batch  131/141   train_loss = 6.156\n",
      "Epoch  10 Batch   90/141   train_loss = 6.141\n",
      "Epoch  11 Batch   49/141   train_loss = 5.984\n",
      "Epoch  12 Batch    8/141   train_loss = 5.699\n",
      "Epoch  12 Batch  108/141   train_loss = 5.483\n",
      "Epoch  13 Batch   67/141   train_loss = 5.378\n",
      "Epoch  14 Batch   26/141   train_loss = 5.153\n",
      "Epoch  14 Batch  126/141   train_loss = 4.995\n",
      "Epoch  15 Batch   85/141   train_loss = 5.023\n",
      "Epoch  16 Batch   44/141   train_loss = 4.960\n",
      "Epoch  17 Batch    3/141   train_loss = 4.846\n",
      "Epoch  17 Batch  103/141   train_loss = 4.712\n",
      "Epoch  18 Batch   62/141   train_loss = 4.809\n",
      "Epoch  19 Batch   21/141   train_loss = 4.707\n",
      "Epoch  19 Batch  121/141   train_loss = 4.693\n",
      "Epoch  20 Batch   80/141   train_loss = 4.652\n",
      "Epoch  21 Batch   39/141   train_loss = 4.605\n",
      "Epoch  21 Batch  139/141   train_loss = 4.479\n",
      "Epoch  22 Batch   98/141   train_loss = 4.508\n",
      "Epoch  23 Batch   57/141   train_loss = 4.556\n",
      "Epoch  24 Batch   16/141   train_loss = 4.487\n",
      "Epoch  24 Batch  116/141   train_loss = 4.388\n",
      "Epoch  25 Batch   75/141   train_loss = 4.399\n",
      "Epoch  26 Batch   34/141   train_loss = 4.331\n",
      "Epoch  26 Batch  134/141   train_loss = 4.312\n",
      "Epoch  27 Batch   93/141   train_loss = 4.326\n",
      "Epoch  28 Batch   52/141   train_loss = 4.255\n",
      "Epoch  29 Batch   11/141   train_loss = 4.261\n",
      "Epoch  29 Batch  111/141   train_loss = 4.217\n",
      "Epoch  30 Batch   70/141   train_loss = 4.226\n",
      "Epoch  31 Batch   29/141   train_loss = 4.239\n",
      "Epoch  31 Batch  129/141   train_loss = 4.115\n",
      "Epoch  32 Batch   88/141   train_loss = 4.154\n",
      "Epoch  33 Batch   47/141   train_loss = 4.097\n",
      "Epoch  34 Batch    6/141   train_loss = 4.122\n",
      "Epoch  34 Batch  106/141   train_loss = 4.101\n",
      "Epoch  35 Batch   65/141   train_loss = 4.033\n",
      "Epoch  36 Batch   24/141   train_loss = 3.987\n",
      "Epoch  36 Batch  124/141   train_loss = 4.012\n",
      "Epoch  37 Batch   83/141   train_loss = 3.996\n",
      "Epoch  38 Batch   42/141   train_loss = 3.971\n",
      "Epoch  39 Batch    1/141   train_loss = 3.917\n",
      "Epoch  39 Batch  101/141   train_loss = 3.892\n",
      "Epoch  40 Batch   60/141   train_loss = 3.943\n",
      "Epoch  41 Batch   19/141   train_loss = 3.902\n",
      "Epoch  41 Batch  119/141   train_loss = 3.899\n",
      "Epoch  42 Batch   78/141   train_loss = 3.952\n",
      "Epoch  43 Batch   37/141   train_loss = 3.811\n",
      "Epoch  43 Batch  137/141   train_loss = 3.799\n",
      "Epoch  44 Batch   96/141   train_loss = 3.812\n",
      "Epoch  45 Batch   55/141   train_loss = 3.813\n",
      "Epoch  46 Batch   14/141   train_loss = 3.733\n",
      "Epoch  46 Batch  114/141   train_loss = 3.744\n",
      "Epoch  47 Batch   73/141   train_loss = 3.781\n",
      "Epoch  48 Batch   32/141   train_loss = 3.708\n",
      "Epoch  48 Batch  132/141   train_loss = 3.626\n",
      "Epoch  49 Batch   91/141   train_loss = 3.648\n",
      "Epoch  50 Batch   50/141   train_loss = 3.624\n",
      "Epoch  51 Batch    9/141   train_loss = 3.573\n",
      "Epoch  51 Batch  109/141   train_loss = 3.649\n",
      "Epoch  52 Batch   68/141   train_loss = 3.590\n",
      "Epoch  53 Batch   27/141   train_loss = 3.631\n",
      "Epoch  53 Batch  127/141   train_loss = 3.639\n",
      "Epoch  54 Batch   86/141   train_loss = 3.566\n",
      "Epoch  55 Batch   45/141   train_loss = 3.567\n",
      "Epoch  56 Batch    4/141   train_loss = 3.581\n",
      "Epoch  56 Batch  104/141   train_loss = 3.581\n",
      "Epoch  57 Batch   63/141   train_loss = 3.523\n",
      "Epoch  58 Batch   22/141   train_loss = 3.480\n",
      "Epoch  58 Batch  122/141   train_loss = 3.467\n",
      "Epoch  59 Batch   81/141   train_loss = 3.509\n",
      "Epoch  60 Batch   40/141   train_loss = 3.498\n",
      "Epoch  60 Batch  140/141   train_loss = 3.428\n",
      "Epoch  61 Batch   99/141   train_loss = 3.442\n",
      "Epoch  62 Batch   58/141   train_loss = 3.482\n",
      "Epoch  63 Batch   17/141   train_loss = 3.465\n",
      "Epoch  63 Batch  117/141   train_loss = 3.401\n",
      "Epoch  64 Batch   76/141   train_loss = 3.339\n",
      "Epoch  65 Batch   35/141   train_loss = 3.361\n",
      "Epoch  65 Batch  135/141   train_loss = 3.415\n",
      "Epoch  66 Batch   94/141   train_loss = 3.348\n",
      "Epoch  67 Batch   53/141   train_loss = 3.356\n",
      "Epoch  68 Batch   12/141   train_loss = 3.248\n",
      "Epoch  68 Batch  112/141   train_loss = 3.372\n",
      "Epoch  69 Batch   71/141   train_loss = 3.335\n",
      "Epoch  70 Batch   30/141   train_loss = 3.278\n",
      "Epoch  70 Batch  130/141   train_loss = 3.309\n",
      "Epoch  71 Batch   89/141   train_loss = 3.230\n",
      "Epoch  72 Batch   48/141   train_loss = 3.323\n",
      "Epoch  73 Batch    7/141   train_loss = 3.306\n",
      "Epoch  73 Batch  107/141   train_loss = 3.283\n",
      "Epoch  74 Batch   66/141   train_loss = 3.204\n",
      "Epoch  75 Batch   25/141   train_loss = 3.265\n",
      "Epoch  75 Batch  125/141   train_loss = 3.199\n",
      "Epoch  76 Batch   84/141   train_loss = 3.311\n",
      "Epoch  77 Batch   43/141   train_loss = 3.178\n",
      "Epoch  78 Batch    2/141   train_loss = 3.207\n",
      "Epoch  78 Batch  102/141   train_loss = 3.265\n",
      "Epoch  79 Batch   61/141   train_loss = 3.233\n",
      "Epoch  80 Batch   20/141   train_loss = 3.214\n",
      "Epoch  80 Batch  120/141   train_loss = 3.201\n",
      "Epoch  81 Batch   79/141   train_loss = 3.214\n",
      "Epoch  82 Batch   38/141   train_loss = 3.058\n",
      "Epoch  82 Batch  138/141   train_loss = 3.199\n",
      "Epoch  83 Batch   97/141   train_loss = 3.140\n",
      "Epoch  84 Batch   56/141   train_loss = 3.209\n",
      "Epoch  85 Batch   15/141   train_loss = 3.138\n",
      "Epoch  85 Batch  115/141   train_loss = 3.147\n",
      "Epoch  86 Batch   74/141   train_loss = 3.123\n",
      "Epoch  87 Batch   33/141   train_loss = 3.093\n",
      "Epoch  87 Batch  133/141   train_loss = 3.124\n",
      "Epoch  88 Batch   92/141   train_loss = 3.172\n",
      "Epoch  89 Batch   51/141   train_loss = 3.060\n",
      "Epoch  90 Batch   10/141   train_loss = 3.143\n",
      "Epoch  90 Batch  110/141   train_loss = 3.027\n",
      "Epoch  91 Batch   69/141   train_loss = 3.000\n",
      "Epoch  92 Batch   28/141   train_loss = 3.100\n",
      "Epoch  92 Batch  128/141   train_loss = 3.061\n",
      "Epoch  93 Batch   87/141   train_loss = 3.025\n",
      "Epoch  94 Batch   46/141   train_loss = 3.029\n",
      "Epoch  95 Batch    5/141   train_loss = 3.028\n",
      "Epoch  95 Batch  105/141   train_loss = 3.044\n",
      "Epoch  96 Batch   64/141   train_loss = 3.019\n",
      "Epoch  97 Batch   23/141   train_loss = 3.014\n",
      "Epoch  97 Batch  123/141   train_loss = 3.021\n",
      "Epoch  98 Batch   82/141   train_loss = 3.012\n",
      "Epoch  99 Batch   41/141   train_loss = 2.951\n",
      "Model Trained and Saved - Epoch: 100\n",
      "Epoch 100 Batch    0/141   train_loss = 3.027\n",
      "Epoch 100 Batch  100/141   train_loss = 3.015\n",
      "Epoch 101 Batch   59/141   train_loss = 2.985\n",
      "Epoch 102 Batch   18/141   train_loss = 2.888\n",
      "Epoch 102 Batch  118/141   train_loss = 2.969\n",
      "Epoch 103 Batch   77/141   train_loss = 3.014\n",
      "Epoch 104 Batch   36/141   train_loss = 2.929\n",
      "Epoch 104 Batch  136/141   train_loss = 2.910\n",
      "Epoch 105 Batch   95/141   train_loss = 2.852\n",
      "Epoch 106 Batch   54/141   train_loss = 2.931\n",
      "Epoch 107 Batch   13/141   train_loss = 2.823\n",
      "Epoch 107 Batch  113/141   train_loss = 2.873\n",
      "Epoch 108 Batch   72/141   train_loss = 2.939\n",
      "Epoch 109 Batch   31/141   train_loss = 2.871\n",
      "Epoch 109 Batch  131/141   train_loss = 2.821\n",
      "Epoch 110 Batch   90/141   train_loss = 2.886\n",
      "Epoch 111 Batch   49/141   train_loss = 2.831\n",
      "Epoch 112 Batch    8/141   train_loss = 2.861\n",
      "Epoch 112 Batch  108/141   train_loss = 2.903\n",
      "Epoch 113 Batch   67/141   train_loss = 2.860\n",
      "Epoch 114 Batch   26/141   train_loss = 2.804\n",
      "Epoch 114 Batch  126/141   train_loss = 2.792\n",
      "Epoch 115 Batch   85/141   train_loss = 2.794\n",
      "Epoch 116 Batch   44/141   train_loss = 2.740\n",
      "Epoch 117 Batch    3/141   train_loss = 2.742\n",
      "Epoch 117 Batch  103/141   train_loss = 2.848\n",
      "Epoch 118 Batch   62/141   train_loss = 2.843\n",
      "Epoch 119 Batch   21/141   train_loss = 2.788\n",
      "Epoch 119 Batch  121/141   train_loss = 2.821\n",
      "Epoch 120 Batch   80/141   train_loss = 2.810\n",
      "Epoch 121 Batch   39/141   train_loss = 2.724\n",
      "Epoch 121 Batch  139/141   train_loss = 2.684\n",
      "Epoch 122 Batch   98/141   train_loss = 2.811\n",
      "Epoch 123 Batch   57/141   train_loss = 2.745\n",
      "Epoch 124 Batch   16/141   train_loss = 2.696\n",
      "Epoch 124 Batch  116/141   train_loss = 2.733\n",
      "Epoch 125 Batch   75/141   train_loss = 2.732\n",
      "Epoch 126 Batch   34/141   train_loss = 2.703\n",
      "Epoch 126 Batch  134/141   train_loss = 2.711\n",
      "Epoch 127 Batch   93/141   train_loss = 2.819\n",
      "Epoch 128 Batch   52/141   train_loss = 2.702\n",
      "Epoch 129 Batch   11/141   train_loss = 2.678\n",
      "Epoch 129 Batch  111/141   train_loss = 2.696\n",
      "Epoch 130 Batch   70/141   train_loss = 2.652\n",
      "Epoch 131 Batch   29/141   train_loss = 2.722\n",
      "Epoch 131 Batch  129/141   train_loss = 2.665\n",
      "Epoch 132 Batch   88/141   train_loss = 2.659\n",
      "Epoch 133 Batch   47/141   train_loss = 2.572\n",
      "Epoch 134 Batch    6/141   train_loss = 2.683\n",
      "Epoch 134 Batch  106/141   train_loss = 2.732\n",
      "Epoch 135 Batch   65/141   train_loss = 2.639\n",
      "Epoch 136 Batch   24/141   train_loss = 2.568\n",
      "Epoch 136 Batch  124/141   train_loss = 2.645\n",
      "Epoch 137 Batch   83/141   train_loss = 2.654\n",
      "Epoch 138 Batch   42/141   train_loss = 2.588\n",
      "Epoch 139 Batch    1/141   train_loss = 2.575\n",
      "Epoch 139 Batch  101/141   train_loss = 2.650\n",
      "Epoch 140 Batch   60/141   train_loss = 2.579\n",
      "Epoch 141 Batch   19/141   train_loss = 2.649\n",
      "Epoch 141 Batch  119/141   train_loss = 2.619\n",
      "Epoch 142 Batch   78/141   train_loss = 2.664\n",
      "Epoch 143 Batch   37/141   train_loss = 2.573\n",
      "Epoch 143 Batch  137/141   train_loss = 2.557\n",
      "Epoch 144 Batch   96/141   train_loss = 2.627\n",
      "Epoch 145 Batch   55/141   train_loss = 2.565\n",
      "Epoch 146 Batch   14/141   train_loss = 2.519\n",
      "Epoch 146 Batch  114/141   train_loss = 2.542\n",
      "Epoch 147 Batch   73/141   train_loss = 2.555\n",
      "Epoch 148 Batch   32/141   train_loss = 2.511\n",
      "Epoch 148 Batch  132/141   train_loss = 2.545\n",
      "Epoch 149 Batch   91/141   train_loss = 2.463\n",
      "Epoch 150 Batch   50/141   train_loss = 2.510\n",
      "Epoch 151 Batch    9/141   train_loss = 2.497\n",
      "Epoch 151 Batch  109/141   train_loss = 2.517\n",
      "Epoch 152 Batch   68/141   train_loss = 2.469\n",
      "Epoch 153 Batch   27/141   train_loss = 2.543\n",
      "Epoch 153 Batch  127/141   train_loss = 2.548\n",
      "Epoch 154 Batch   86/141   train_loss = 2.495\n",
      "Epoch 155 Batch   45/141   train_loss = 2.529\n",
      "Epoch 156 Batch    4/141   train_loss = 2.510\n",
      "Epoch 156 Batch  104/141   train_loss = 2.517\n",
      "Epoch 157 Batch   63/141   train_loss = 2.431\n",
      "Epoch 158 Batch   22/141   train_loss = 2.403\n",
      "Epoch 158 Batch  122/141   train_loss = 2.517\n",
      "Epoch 159 Batch   81/141   train_loss = 2.504\n",
      "Epoch 160 Batch   40/141   train_loss = 2.394\n",
      "Epoch 160 Batch  140/141   train_loss = 2.391\n",
      "Epoch 161 Batch   99/141   train_loss = 2.495\n",
      "Epoch 162 Batch   58/141   train_loss = 2.421\n",
      "Epoch 163 Batch   17/141   train_loss = 2.424\n",
      "Epoch 163 Batch  117/141   train_loss = 2.416\n",
      "Epoch 164 Batch   76/141   train_loss = 2.401\n",
      "Epoch 165 Batch   35/141   train_loss = 2.369\n",
      "Epoch 165 Batch  135/141   train_loss = 2.497\n",
      "Epoch 166 Batch   94/141   train_loss = 2.404\n",
      "Epoch 167 Batch   53/141   train_loss = 2.414\n",
      "Epoch 168 Batch   12/141   train_loss = 2.347\n",
      "Epoch 168 Batch  112/141   train_loss = 2.437\n",
      "Epoch 169 Batch   71/141   train_loss = 2.438\n",
      "Epoch 170 Batch   30/141   train_loss = 2.381\n",
      "Epoch 170 Batch  130/141   train_loss = 2.415\n",
      "Epoch 171 Batch   89/141   train_loss = 2.380\n",
      "Epoch 172 Batch   48/141   train_loss = 2.367\n",
      "Epoch 173 Batch    7/141   train_loss = 2.409\n",
      "Epoch 173 Batch  107/141   train_loss = 2.491\n",
      "Epoch 174 Batch   66/141   train_loss = 2.311\n",
      "Epoch 175 Batch   25/141   train_loss = 2.425\n",
      "Epoch 175 Batch  125/141   train_loss = 2.437\n",
      "Epoch 176 Batch   84/141   train_loss = 2.428\n",
      "Epoch 177 Batch   43/141   train_loss = 2.375\n",
      "Epoch 178 Batch    2/141   train_loss = 2.431\n",
      "Epoch 178 Batch  102/141   train_loss = 2.393\n",
      "Epoch 179 Batch   61/141   train_loss = 2.295\n",
      "Epoch 180 Batch   20/141   train_loss = 2.376\n",
      "Epoch 180 Batch  120/141   train_loss = 2.367\n",
      "Epoch 181 Batch   79/141   train_loss = 2.315\n",
      "Epoch 182 Batch   38/141   train_loss = 2.283\n",
      "Epoch 182 Batch  138/141   train_loss = 2.385\n",
      "Epoch 183 Batch   97/141   train_loss = 2.303\n",
      "Epoch 184 Batch   56/141   train_loss = 2.327\n",
      "Epoch 185 Batch   15/141   train_loss = 2.340\n",
      "Epoch 185 Batch  115/141   train_loss = 2.308\n",
      "Epoch 186 Batch   74/141   train_loss = 2.258\n",
      "Epoch 187 Batch   33/141   train_loss = 2.331\n",
      "Epoch 187 Batch  133/141   train_loss = 2.258\n",
      "Epoch 188 Batch   92/141   train_loss = 2.354\n",
      "Epoch 189 Batch   51/141   train_loss = 2.280\n",
      "Epoch 190 Batch   10/141   train_loss = 2.383\n",
      "Epoch 190 Batch  110/141   train_loss = 2.247\n",
      "Epoch 191 Batch   69/141   train_loss = 2.245\n",
      "Epoch 192 Batch   28/141   train_loss = 2.293\n",
      "Epoch 192 Batch  128/141   train_loss = 2.323\n",
      "Epoch 193 Batch   87/141   train_loss = 2.255\n",
      "Epoch 194 Batch   46/141   train_loss = 2.343\n",
      "Epoch 195 Batch    5/141   train_loss = 2.342\n",
      "Epoch 195 Batch  105/141   train_loss = 2.286\n",
      "Epoch 196 Batch   64/141   train_loss = 2.336\n",
      "Epoch 197 Batch   23/141   train_loss = 2.285\n",
      "Epoch 197 Batch  123/141   train_loss = 2.287\n",
      "Epoch 198 Batch   82/141   train_loss = 2.247\n",
      "Epoch 199 Batch   41/141   train_loss = 2.195\n",
      "Model Trained and Saved - Epoch: 200\n",
      "Epoch 200 Batch    0/141   train_loss = 2.289\n",
      "Epoch 200 Batch  100/141   train_loss = 2.244\n",
      "Epoch 201 Batch   59/141   train_loss = 2.228\n",
      "Epoch 202 Batch   18/141   train_loss = 2.243\n",
      "Epoch 202 Batch  118/141   train_loss = 2.244\n",
      "Epoch 203 Batch   77/141   train_loss = 2.263\n",
      "Epoch 204 Batch   36/141   train_loss = 2.228\n",
      "Epoch 204 Batch  136/141   train_loss = 2.190\n",
      "Epoch 205 Batch   95/141   train_loss = 2.169\n",
      "Epoch 206 Batch   54/141   train_loss = 2.255\n",
      "Epoch 207 Batch   13/141   train_loss = 2.125\n",
      "Epoch 207 Batch  113/141   train_loss = 2.178\n",
      "Epoch 208 Batch   72/141   train_loss = 2.276\n",
      "Epoch 209 Batch   31/141   train_loss = 2.139\n",
      "Epoch 209 Batch  131/141   train_loss = 2.200\n",
      "Epoch 210 Batch   90/141   train_loss = 2.217\n",
      "Epoch 211 Batch   49/141   train_loss = 2.106\n",
      "Epoch 212 Batch    8/141   train_loss = 2.206\n",
      "Epoch 212 Batch  108/141   train_loss = 2.222\n",
      "Epoch 213 Batch   67/141   train_loss = 2.133\n",
      "Epoch 214 Batch   26/141   train_loss = 2.100\n",
      "Epoch 214 Batch  126/141   train_loss = 2.150\n",
      "Epoch 215 Batch   85/141   train_loss = 2.065\n",
      "Epoch 216 Batch   44/141   train_loss = 2.078\n",
      "Epoch 217 Batch    3/141   train_loss = 2.069\n",
      "Epoch 217 Batch  103/141   train_loss = 2.134\n",
      "Epoch 218 Batch   62/141   train_loss = 2.131\n",
      "Epoch 219 Batch   21/141   train_loss = 2.149\n",
      "Epoch 219 Batch  121/141   train_loss = 2.137\n",
      "Epoch 220 Batch   80/141   train_loss = 2.117\n",
      "Epoch 221 Batch   39/141   train_loss = 2.049\n",
      "Epoch 221 Batch  139/141   train_loss = 2.007\n",
      "Epoch 222 Batch   98/141   train_loss = 2.104\n",
      "Epoch 223 Batch   57/141   train_loss = 2.020\n",
      "Epoch 224 Batch   16/141   train_loss = 2.052\n",
      "Epoch 224 Batch  116/141   train_loss = 2.074\n",
      "Epoch 225 Batch   75/141   train_loss = 2.045\n",
      "Epoch 226 Batch   34/141   train_loss = 2.047\n",
      "Epoch 226 Batch  134/141   train_loss = 2.043\n",
      "Epoch 227 Batch   93/141   train_loss = 2.124\n",
      "Epoch 228 Batch   52/141   train_loss = 2.064\n",
      "Epoch 229 Batch   11/141   train_loss = 2.074\n",
      "Epoch 229 Batch  111/141   train_loss = 2.003\n",
      "Epoch 230 Batch   70/141   train_loss = 1.964\n",
      "Epoch 231 Batch   29/141   train_loss = 2.066\n",
      "Epoch 231 Batch  129/141   train_loss = 2.065\n",
      "Epoch 232 Batch   88/141   train_loss = 2.020\n",
      "Epoch 233 Batch   47/141   train_loss = 1.977\n",
      "Epoch 234 Batch    6/141   train_loss = 2.101\n",
      "Epoch 234 Batch  106/141   train_loss = 2.063\n",
      "Epoch 235 Batch   65/141   train_loss = 2.018\n",
      "Epoch 236 Batch   24/141   train_loss = 2.024\n",
      "Epoch 236 Batch  124/141   train_loss = 2.078\n",
      "Epoch 237 Batch   83/141   train_loss = 2.009\n",
      "Epoch 238 Batch   42/141   train_loss = 1.935\n",
      "Epoch 239 Batch    1/141   train_loss = 2.011\n",
      "Epoch 239 Batch  101/141   train_loss = 2.043\n",
      "Epoch 240 Batch   60/141   train_loss = 1.974\n",
      "Epoch 241 Batch   19/141   train_loss = 2.058\n",
      "Epoch 241 Batch  119/141   train_loss = 1.982\n",
      "Epoch 242 Batch   78/141   train_loss = 2.009\n",
      "Epoch 243 Batch   37/141   train_loss = 1.891\n",
      "Epoch 243 Batch  137/141   train_loss = 1.961\n",
      "Epoch 244 Batch   96/141   train_loss = 1.994\n",
      "Epoch 245 Batch   55/141   train_loss = 1.937\n",
      "Epoch 246 Batch   14/141   train_loss = 1.960\n",
      "Epoch 246 Batch  114/141   train_loss = 1.907\n",
      "Epoch 247 Batch   73/141   train_loss = 1.903\n",
      "Epoch 248 Batch   32/141   train_loss = 1.946\n",
      "Epoch 248 Batch  132/141   train_loss = 1.916\n",
      "Epoch 249 Batch   91/141   train_loss = 1.876\n",
      "Epoch 250 Batch   50/141   train_loss = 1.947\n",
      "Epoch 251 Batch    9/141   train_loss = 1.905\n",
      "Epoch 251 Batch  109/141   train_loss = 1.938\n",
      "Epoch 252 Batch   68/141   train_loss = 1.904\n",
      "Epoch 253 Batch   27/141   train_loss = 1.943\n",
      "Epoch 253 Batch  127/141   train_loss = 1.998\n",
      "Epoch 254 Batch   86/141   train_loss = 1.870\n",
      "Epoch 255 Batch   45/141   train_loss = 1.896\n",
      "Epoch 256 Batch    4/141   train_loss = 1.961\n",
      "Epoch 256 Batch  104/141   train_loss = 1.938\n",
      "Epoch 257 Batch   63/141   train_loss = 1.844\n",
      "Epoch 258 Batch   22/141   train_loss = 1.882\n",
      "Epoch 258 Batch  122/141   train_loss = 1.923\n",
      "Epoch 259 Batch   81/141   train_loss = 1.894\n",
      "Epoch 260 Batch   40/141   train_loss = 1.911\n",
      "Epoch 260 Batch  140/141   train_loss = 1.852\n",
      "Epoch 261 Batch   99/141   train_loss = 1.929\n",
      "Epoch 262 Batch   58/141   train_loss = 1.914\n",
      "Epoch 263 Batch   17/141   train_loss = 1.923\n",
      "Epoch 263 Batch  117/141   train_loss = 1.929\n",
      "Epoch 264 Batch   76/141   train_loss = 1.901\n",
      "Epoch 265 Batch   35/141   train_loss = 1.874\n",
      "Epoch 265 Batch  135/141   train_loss = 2.012\n",
      "Epoch 266 Batch   94/141   train_loss = 1.883\n",
      "Epoch 267 Batch   53/141   train_loss = 1.893\n",
      "Epoch 268 Batch   12/141   train_loss = 1.897\n",
      "Epoch 268 Batch  112/141   train_loss = 1.871\n",
      "Epoch 269 Batch   71/141   train_loss = 1.886\n",
      "Epoch 270 Batch   30/141   train_loss = 1.962\n",
      "Epoch 270 Batch  130/141   train_loss = 1.838\n",
      "Epoch 271 Batch   89/141   train_loss = 1.811\n",
      "Epoch 272 Batch   48/141   train_loss = 1.883\n",
      "Epoch 273 Batch    7/141   train_loss = 1.876\n",
      "Epoch 273 Batch  107/141   train_loss = 1.888\n",
      "Epoch 274 Batch   66/141   train_loss = 1.868\n",
      "Epoch 275 Batch   25/141   train_loss = 1.919\n",
      "Epoch 275 Batch  125/141   train_loss = 1.883\n",
      "Epoch 276 Batch   84/141   train_loss = 2.007\n",
      "Epoch 277 Batch   43/141   train_loss = 1.877\n",
      "Epoch 278 Batch    2/141   train_loss = 1.888\n",
      "Epoch 278 Batch  102/141   train_loss = 1.967\n",
      "Epoch 279 Batch   61/141   train_loss = 1.886\n",
      "Epoch 280 Batch   20/141   train_loss = 1.897\n",
      "Epoch 280 Batch  120/141   train_loss = 1.880\n",
      "Epoch 281 Batch   79/141   train_loss = 1.923\n",
      "Epoch 282 Batch   38/141   train_loss = 1.815\n",
      "Epoch 282 Batch  138/141   train_loss = 1.894\n",
      "Epoch 283 Batch   97/141   train_loss = 1.896\n",
      "Epoch 284 Batch   56/141   train_loss = 1.846\n",
      "Epoch 285 Batch   15/141   train_loss = 1.873\n",
      "Epoch 285 Batch  115/141   train_loss = 1.871\n",
      "Epoch 286 Batch   74/141   train_loss = 1.795\n",
      "Epoch 287 Batch   33/141   train_loss = 1.847\n",
      "Epoch 287 Batch  133/141   train_loss = 1.843\n",
      "Epoch 288 Batch   92/141   train_loss = 1.914\n",
      "Epoch 289 Batch   51/141   train_loss = 1.838\n",
      "Epoch 290 Batch   10/141   train_loss = 1.890\n",
      "Epoch 290 Batch  110/141   train_loss = 1.883\n",
      "Epoch 291 Batch   69/141   train_loss = 1.823\n",
      "Epoch 292 Batch   28/141   train_loss = 1.818\n",
      "Epoch 292 Batch  128/141   train_loss = 1.914\n",
      "Epoch 293 Batch   87/141   train_loss = 1.841\n",
      "Epoch 294 Batch   46/141   train_loss = 1.845\n",
      "Epoch 295 Batch    5/141   train_loss = 1.914\n",
      "Epoch 295 Batch  105/141   train_loss = 1.867\n",
      "Epoch 296 Batch   64/141   train_loss = 1.867\n",
      "Epoch 297 Batch   23/141   train_loss = 1.853\n",
      "Epoch 297 Batch  123/141   train_loss = 1.927\n",
      "Epoch 298 Batch   82/141   train_loss = 1.804\n",
      "Epoch 299 Batch   41/141   train_loss = 1.740\n",
      "Model Trained and Saved - Epoch: 300\n",
      "Epoch 300 Batch    0/141   train_loss = 1.890\n",
      "Epoch 300 Batch  100/141   train_loss = 1.859\n",
      "Epoch 301 Batch   59/141   train_loss = 1.789\n",
      "Epoch 302 Batch   18/141   train_loss = 1.815\n",
      "Epoch 302 Batch  118/141   train_loss = 1.838\n",
      "Epoch 303 Batch   77/141   train_loss = 1.828\n",
      "Epoch 304 Batch   36/141   train_loss = 1.790\n",
      "Epoch 304 Batch  136/141   train_loss = 1.841\n",
      "Epoch 305 Batch   95/141   train_loss = 1.808\n",
      "Epoch 306 Batch   54/141   train_loss = 1.797\n",
      "Epoch 307 Batch   13/141   train_loss = 1.749\n",
      "Epoch 307 Batch  113/141   train_loss = 1.776\n",
      "Epoch 308 Batch   72/141   train_loss = 1.829\n",
      "Epoch 309 Batch   31/141   train_loss = 1.726\n",
      "Epoch 309 Batch  131/141   train_loss = 1.764\n",
      "Epoch 310 Batch   90/141   train_loss = 1.814\n",
      "Epoch 311 Batch   49/141   train_loss = 1.696\n",
      "Epoch 312 Batch    8/141   train_loss = 1.852\n",
      "Epoch 312 Batch  108/141   train_loss = 1.855\n",
      "Epoch 313 Batch   67/141   train_loss = 1.764\n",
      "Epoch 314 Batch   26/141   train_loss = 1.792\n",
      "Epoch 314 Batch  126/141   train_loss = 1.819\n",
      "Epoch 315 Batch   85/141   train_loss = 1.721\n",
      "Epoch 316 Batch   44/141   train_loss = 1.690\n",
      "Epoch 317 Batch    3/141   train_loss = 1.720\n",
      "Epoch 317 Batch  103/141   train_loss = 1.761\n",
      "Epoch 318 Batch   62/141   train_loss = 1.707\n",
      "Epoch 319 Batch   21/141   train_loss = 1.747\n",
      "Epoch 319 Batch  121/141   train_loss = 1.739\n",
      "Epoch 320 Batch   80/141   train_loss = 1.699\n",
      "Epoch 321 Batch   39/141   train_loss = 1.709\n",
      "Epoch 321 Batch  139/141   train_loss = 1.644\n",
      "Epoch 322 Batch   98/141   train_loss = 1.758\n",
      "Epoch 323 Batch   57/141   train_loss = 1.672\n",
      "Epoch 324 Batch   16/141   train_loss = 1.716\n",
      "Epoch 324 Batch  116/141   train_loss = 1.720\n",
      "Epoch 325 Batch   75/141   train_loss = 1.694\n",
      "Epoch 326 Batch   34/141   train_loss = 1.745\n",
      "Epoch 326 Batch  134/141   train_loss = 1.721\n",
      "Epoch 327 Batch   93/141   train_loss = 1.745\n",
      "Epoch 328 Batch   52/141   train_loss = 1.742\n",
      "Epoch 329 Batch   11/141   train_loss = 1.784\n",
      "Epoch 329 Batch  111/141   train_loss = 1.681\n",
      "Epoch 330 Batch   70/141   train_loss = 1.651\n",
      "Epoch 331 Batch   29/141   train_loss = 1.781\n",
      "Epoch 331 Batch  129/141   train_loss = 1.722\n",
      "Epoch 332 Batch   88/141   train_loss = 1.686\n",
      "Epoch 333 Batch   47/141   train_loss = 1.656\n",
      "Epoch 334 Batch    6/141   train_loss = 1.717\n",
      "Epoch 334 Batch  106/141   train_loss = 1.746\n",
      "Epoch 335 Batch   65/141   train_loss = 1.628\n",
      "Epoch 336 Batch   24/141   train_loss = 1.697\n",
      "Epoch 336 Batch  124/141   train_loss = 1.722\n",
      "Epoch 337 Batch   83/141   train_loss = 1.681\n",
      "Epoch 338 Batch   42/141   train_loss = 1.627\n",
      "Epoch 339 Batch    1/141   train_loss = 1.676\n",
      "Epoch 339 Batch  101/141   train_loss = 1.728\n",
      "Epoch 340 Batch   60/141   train_loss = 1.667\n",
      "Epoch 341 Batch   19/141   train_loss = 1.712\n",
      "Epoch 341 Batch  119/141   train_loss = 1.688\n",
      "Epoch 342 Batch   78/141   train_loss = 1.698\n",
      "Epoch 343 Batch   37/141   train_loss = 1.612\n",
      "Epoch 343 Batch  137/141   train_loss = 1.657\n",
      "Epoch 344 Batch   96/141   train_loss = 1.653\n",
      "Epoch 345 Batch   55/141   train_loss = 1.636\n",
      "Epoch 346 Batch   14/141   train_loss = 1.629\n",
      "Epoch 346 Batch  114/141   train_loss = 1.607\n",
      "Epoch 347 Batch   73/141   train_loss = 1.647\n",
      "Epoch 348 Batch   32/141   train_loss = 1.629\n",
      "Epoch 348 Batch  132/141   train_loss = 1.602\n",
      "Epoch 349 Batch   91/141   train_loss = 1.596\n",
      "Epoch 350 Batch   50/141   train_loss = 1.606\n",
      "Epoch 351 Batch    9/141   train_loss = 1.577\n",
      "Epoch 351 Batch  109/141   train_loss = 1.643\n",
      "Epoch 352 Batch   68/141   train_loss = 1.615\n",
      "Epoch 353 Batch   27/141   train_loss = 1.607\n",
      "Epoch 353 Batch  127/141   train_loss = 1.677\n",
      "Epoch 354 Batch   86/141   train_loss = 1.661\n",
      "Epoch 355 Batch   45/141   train_loss = 1.604\n",
      "Epoch 356 Batch    4/141   train_loss = 1.664\n",
      "Epoch 356 Batch  104/141   train_loss = 1.678\n",
      "Epoch 357 Batch   63/141   train_loss = 1.551\n",
      "Epoch 358 Batch   22/141   train_loss = 1.555\n",
      "Epoch 358 Batch  122/141   train_loss = 1.670\n",
      "Epoch 359 Batch   81/141   train_loss = 1.587\n",
      "Epoch 360 Batch   40/141   train_loss = 1.589\n",
      "Epoch 360 Batch  140/141   train_loss = 1.550\n",
      "Epoch 361 Batch   99/141   train_loss = 1.639\n",
      "Epoch 362 Batch   58/141   train_loss = 1.574\n",
      "Epoch 363 Batch   17/141   train_loss = 1.597\n",
      "Epoch 363 Batch  117/141   train_loss = 1.618\n",
      "Epoch 364 Batch   76/141   train_loss = 1.576\n",
      "Epoch 365 Batch   35/141   train_loss = 1.559\n",
      "Epoch 365 Batch  135/141   train_loss = 1.684\n",
      "Epoch 366 Batch   94/141   train_loss = 1.595\n",
      "Epoch 367 Batch   53/141   train_loss = 1.543\n",
      "Epoch 368 Batch   12/141   train_loss = 1.545\n",
      "Epoch 368 Batch  112/141   train_loss = 1.651\n",
      "Epoch 369 Batch   71/141   train_loss = 1.586\n",
      "Epoch 370 Batch   30/141   train_loss = 1.609\n",
      "Epoch 370 Batch  130/141   train_loss = 1.641\n",
      "Epoch 371 Batch   89/141   train_loss = 1.552\n",
      "Epoch 372 Batch   48/141   train_loss = 1.529\n",
      "Epoch 373 Batch    7/141   train_loss = 1.677\n",
      "Epoch 373 Batch  107/141   train_loss = 1.592\n",
      "Epoch 374 Batch   66/141   train_loss = 1.515\n",
      "Epoch 375 Batch   25/141   train_loss = 1.630\n",
      "Epoch 375 Batch  125/141   train_loss = 1.594\n",
      "Epoch 376 Batch   84/141   train_loss = 1.666\n",
      "Epoch 377 Batch   43/141   train_loss = 1.587\n",
      "Epoch 378 Batch    2/141   train_loss = 1.656\n",
      "Epoch 378 Batch  102/141   train_loss = 1.631\n",
      "Epoch 379 Batch   61/141   train_loss = 1.544\n",
      "Epoch 380 Batch   20/141   train_loss = 1.575\n",
      "Epoch 380 Batch  120/141   train_loss = 1.602\n",
      "Epoch 381 Batch   79/141   train_loss = 1.557\n",
      "Epoch 382 Batch   38/141   train_loss = 1.513\n",
      "Epoch 382 Batch  138/141   train_loss = 1.586\n",
      "Epoch 383 Batch   97/141   train_loss = 1.525\n",
      "Epoch 384 Batch   56/141   train_loss = 1.538\n",
      "Epoch 385 Batch   15/141   train_loss = 1.570\n",
      "Epoch 385 Batch  115/141   train_loss = 1.522\n",
      "Epoch 386 Batch   74/141   train_loss = 1.503\n",
      "Epoch 387 Batch   33/141   train_loss = 1.553\n",
      "Epoch 387 Batch  133/141   train_loss = 1.523\n",
      "Epoch 388 Batch   92/141   train_loss = 1.546\n",
      "Epoch 389 Batch   51/141   train_loss = 1.563\n",
      "Epoch 390 Batch   10/141   train_loss = 1.614\n",
      "Epoch 390 Batch  110/141   train_loss = 1.509\n",
      "Epoch 391 Batch   69/141   train_loss = 1.535\n",
      "Epoch 392 Batch   28/141   train_loss = 1.545\n",
      "Epoch 392 Batch  128/141   train_loss = 1.544\n",
      "Epoch 393 Batch   87/141   train_loss = 1.509\n",
      "Epoch 394 Batch   46/141   train_loss = 1.564\n",
      "Epoch 395 Batch    5/141   train_loss = 1.578\n",
      "Epoch 395 Batch  105/141   train_loss = 1.538\n",
      "Epoch 396 Batch   64/141   train_loss = 1.549\n",
      "Epoch 397 Batch   23/141   train_loss = 1.519\n",
      "Epoch 397 Batch  123/141   train_loss = 1.542\n",
      "Epoch 398 Batch   82/141   train_loss = 1.493\n",
      "Epoch 399 Batch   41/141   train_loss = 1.452\n",
      "Model Trained and Saved - Epoch: 400\n",
      "Epoch 400 Batch    0/141   train_loss = 1.563\n",
      "Epoch 400 Batch  100/141   train_loss = 1.493\n",
      "Epoch 401 Batch   59/141   train_loss = 1.482\n",
      "Epoch 402 Batch   18/141   train_loss = 1.505\n",
      "Epoch 402 Batch  118/141   train_loss = 1.484\n",
      "Epoch 403 Batch   77/141   train_loss = 1.547\n",
      "Epoch 404 Batch   36/141   train_loss = 1.493\n",
      "Epoch 404 Batch  136/141   train_loss = 1.518\n",
      "Epoch 405 Batch   95/141   train_loss = 1.482\n",
      "Epoch 406 Batch   54/141   train_loss = 1.475\n",
      "Epoch 407 Batch   13/141   train_loss = 1.500\n",
      "Epoch 407 Batch  113/141   train_loss = 1.420\n",
      "Epoch 408 Batch   72/141   train_loss = 1.505\n",
      "Epoch 409 Batch   31/141   train_loss = 1.466\n",
      "Epoch 409 Batch  131/141   train_loss = 1.444\n",
      "Epoch 410 Batch   90/141   train_loss = 1.536\n",
      "Epoch 411 Batch   49/141   train_loss = 1.452\n",
      "Epoch 412 Batch    8/141   train_loss = 1.515\n",
      "Epoch 412 Batch  108/141   train_loss = 1.519\n",
      "Epoch 413 Batch   67/141   train_loss = 1.444\n",
      "Epoch 414 Batch   26/141   train_loss = 1.460\n",
      "Epoch 414 Batch  126/141   train_loss = 1.508\n",
      "Epoch 415 Batch   85/141   train_loss = 1.398\n",
      "Epoch 416 Batch   44/141   train_loss = 1.454\n",
      "Epoch 417 Batch    3/141   train_loss = 1.436\n",
      "Epoch 417 Batch  103/141   train_loss = 1.468\n",
      "Epoch 418 Batch   62/141   train_loss = 1.466\n",
      "Epoch 419 Batch   21/141   train_loss = 1.454\n",
      "Epoch 419 Batch  121/141   train_loss = 1.441\n",
      "Epoch 420 Batch   80/141   train_loss = 1.452\n",
      "Epoch 421 Batch   39/141   train_loss = 1.431\n",
      "Epoch 421 Batch  139/141   train_loss = 1.380\n",
      "Epoch 422 Batch   98/141   train_loss = 1.463\n",
      "Epoch 423 Batch   57/141   train_loss = 1.366\n",
      "Epoch 424 Batch   16/141   train_loss = 1.436\n",
      "Epoch 424 Batch  116/141   train_loss = 1.397\n",
      "Epoch 425 Batch   75/141   train_loss = 1.387\n",
      "Epoch 426 Batch   34/141   train_loss = 1.465\n",
      "Epoch 426 Batch  134/141   train_loss = 1.379\n",
      "Epoch 427 Batch   93/141   train_loss = 1.443\n",
      "Epoch 428 Batch   52/141   train_loss = 1.448\n",
      "Epoch 429 Batch   11/141   train_loss = 1.427\n",
      "Epoch 429 Batch  111/141   train_loss = 1.376\n",
      "Epoch 430 Batch   70/141   train_loss = 1.383\n",
      "Epoch 431 Batch   29/141   train_loss = 1.415\n",
      "Epoch 431 Batch  129/141   train_loss = 1.423\n",
      "Epoch 432 Batch   88/141   train_loss = 1.435\n",
      "Epoch 433 Batch   47/141   train_loss = 1.337\n",
      "Epoch 434 Batch    6/141   train_loss = 1.431\n",
      "Epoch 434 Batch  106/141   train_loss = 1.391\n",
      "Epoch 435 Batch   65/141   train_loss = 1.340\n",
      "Epoch 436 Batch   24/141   train_loss = 1.397\n",
      "Epoch 436 Batch  124/141   train_loss = 1.374\n",
      "Epoch 437 Batch   83/141   train_loss = 1.409\n",
      "Epoch 438 Batch   42/141   train_loss = 1.368\n",
      "Epoch 439 Batch    1/141   train_loss = 1.373\n",
      "Epoch 439 Batch  101/141   train_loss = 1.441\n",
      "Epoch 440 Batch   60/141   train_loss = 1.365\n",
      "Epoch 441 Batch   19/141   train_loss = 1.409\n",
      "Epoch 441 Batch  119/141   train_loss = 1.383\n",
      "Epoch 442 Batch   78/141   train_loss = 1.390\n",
      "Epoch 443 Batch   37/141   train_loss = 1.349\n",
      "Epoch 443 Batch  137/141   train_loss = 1.367\n",
      "Epoch 444 Batch   96/141   train_loss = 1.382\n",
      "Epoch 445 Batch   55/141   train_loss = 1.310\n",
      "Epoch 446 Batch   14/141   train_loss = 1.376\n",
      "Epoch 446 Batch  114/141   train_loss = 1.401\n",
      "Epoch 447 Batch   73/141   train_loss = 1.343\n",
      "Epoch 448 Batch   32/141   train_loss = 1.379\n",
      "Epoch 448 Batch  132/141   train_loss = 1.409\n",
      "Epoch 449 Batch   91/141   train_loss = 1.288\n",
      "Epoch 450 Batch   50/141   train_loss = 1.392\n",
      "Epoch 451 Batch    9/141   train_loss = 1.382\n",
      "Epoch 451 Batch  109/141   train_loss = 1.318\n",
      "Epoch 452 Batch   68/141   train_loss = 1.346\n",
      "Epoch 453 Batch   27/141   train_loss = 1.371\n",
      "Epoch 453 Batch  127/141   train_loss = 1.420\n",
      "Epoch 454 Batch   86/141   train_loss = 1.325\n",
      "Epoch 455 Batch   45/141   train_loss = 1.349\n",
      "Epoch 456 Batch    4/141   train_loss = 1.474\n",
      "Epoch 456 Batch  104/141   train_loss = 1.301\n",
      "Epoch 457 Batch   63/141   train_loss = 1.273\n",
      "Epoch 458 Batch   22/141   train_loss = 1.402\n",
      "Epoch 458 Batch  122/141   train_loss = 1.308\n",
      "Epoch 459 Batch   81/141   train_loss = 1.340\n",
      "Epoch 460 Batch   40/141   train_loss = 1.413\n",
      "Epoch 460 Batch  140/141   train_loss = 1.298\n",
      "Epoch 461 Batch   99/141   train_loss = 1.348\n",
      "Epoch 462 Batch   58/141   train_loss = 1.359\n",
      "Epoch 463 Batch   17/141   train_loss = 1.362\n",
      "Epoch 463 Batch  117/141   train_loss = 1.332\n",
      "Epoch 464 Batch   76/141   train_loss = 1.337\n",
      "Epoch 465 Batch   35/141   train_loss = 1.346\n",
      "Epoch 465 Batch  135/141   train_loss = 1.406\n",
      "Epoch 466 Batch   94/141   train_loss = 1.344\n",
      "Epoch 467 Batch   53/141   train_loss = 1.339\n",
      "Epoch 468 Batch   12/141   train_loss = 1.314\n",
      "Epoch 468 Batch  112/141   train_loss = 1.325\n",
      "Epoch 469 Batch   71/141   train_loss = 1.327\n",
      "Epoch 470 Batch   30/141   train_loss = 1.398\n",
      "Epoch 470 Batch  130/141   train_loss = 1.308\n",
      "Epoch 471 Batch   89/141   train_loss = 1.295\n",
      "Epoch 472 Batch   48/141   train_loss = 1.368\n",
      "Epoch 473 Batch    7/141   train_loss = 1.331\n",
      "Epoch 473 Batch  107/141   train_loss = 1.309\n",
      "Epoch 474 Batch   66/141   train_loss = 1.315\n",
      "Epoch 475 Batch   25/141   train_loss = 1.350\n",
      "Epoch 475 Batch  125/141   train_loss = 1.325\n",
      "Epoch 476 Batch   84/141   train_loss = 1.417\n",
      "Epoch 477 Batch   43/141   train_loss = 1.337\n",
      "Epoch 478 Batch    2/141   train_loss = 1.338\n",
      "Epoch 478 Batch  102/141   train_loss = 1.357\n",
      "Epoch 479 Batch   61/141   train_loss = 1.327\n",
      "Epoch 480 Batch   20/141   train_loss = 1.295\n",
      "Epoch 480 Batch  120/141   train_loss = 1.332\n",
      "Epoch 481 Batch   79/141   train_loss = 1.333\n",
      "Epoch 482 Batch   38/141   train_loss = 1.295\n",
      "Epoch 482 Batch  138/141   train_loss = 1.332\n",
      "Epoch 483 Batch   97/141   train_loss = 1.343\n",
      "Epoch 484 Batch   56/141   train_loss = 1.345\n",
      "Epoch 485 Batch   15/141   train_loss = 1.331\n",
      "Epoch 485 Batch  115/141   train_loss = 1.310\n",
      "Epoch 486 Batch   74/141   train_loss = 1.343\n",
      "Epoch 487 Batch   33/141   train_loss = 1.336\n",
      "Epoch 487 Batch  133/141   train_loss = 1.408\n",
      "Epoch 488 Batch   92/141   train_loss = 1.388\n",
      "Epoch 489 Batch   51/141   train_loss = 1.327\n",
      "Epoch 490 Batch   10/141   train_loss = 1.470\n",
      "Epoch 490 Batch  110/141   train_loss = 1.298\n",
      "Epoch 491 Batch   69/141   train_loss = 1.333\n",
      "Epoch 492 Batch   28/141   train_loss = 1.416\n",
      "Epoch 492 Batch  128/141   train_loss = 1.337\n",
      "Epoch 493 Batch   87/141   train_loss = 1.357\n",
      "Epoch 494 Batch   46/141   train_loss = 1.388\n",
      "Epoch 495 Batch    5/141   train_loss = 1.361\n",
      "Epoch 495 Batch  105/141   train_loss = 1.403\n",
      "Epoch 496 Batch   64/141   train_loss = 1.315\n",
      "Epoch 497 Batch   23/141   train_loss = 1.343\n",
      "Epoch 497 Batch  123/141   train_loss = 1.365\n",
      "Epoch 498 Batch   82/141   train_loss = 1.250\n",
      "Epoch 499 Batch   41/141   train_loss = 1.319\n",
      "Model Trained and Saved - Epoch: 500\n",
      "Epoch 500 Batch    0/141   train_loss = 1.386\n",
      "Epoch 500 Batch  100/141   train_loss = 1.292\n",
      "Epoch 501 Batch   59/141   train_loss = 1.355\n",
      "Epoch 502 Batch   18/141   train_loss = 1.320\n",
      "Epoch 502 Batch  118/141   train_loss = 1.254\n",
      "Epoch 503 Batch   77/141   train_loss = 1.383\n",
      "Epoch 504 Batch   36/141   train_loss = 1.299\n",
      "Epoch 504 Batch  136/141   train_loss = 1.294\n",
      "Epoch 505 Batch   95/141   train_loss = 1.325\n",
      "Epoch 506 Batch   54/141   train_loss = 1.296\n",
      "Epoch 507 Batch   13/141   train_loss = 1.266\n",
      "Epoch 507 Batch  113/141   train_loss = 1.232\n",
      "Epoch 508 Batch   72/141   train_loss = 1.331\n",
      "Epoch 509 Batch   31/141   train_loss = 1.240\n"
     ]
    }
   ],
   "source": [
    "batches = Vidyabagishnn.get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "# file_name_suffix = \"-lr-{}-epochs-{}-sqe_length-{}-\".format(learning_rate, num_epochs, seq_length)\n",
    "\n",
    "training_log = \"batch_size: {}\\nepochs: {}\\nrnn_layer_size: {}\\nrnn_size: {}\\nembed_dim: {}\\nseq_length: {}\\nlr: {}\\ndropout: {}\\n--------\\n\".format(batch_size, num_epochs, rnn_layer_size, rnn_size, embed_dim, seq_length, learning_rate, dropout)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                current_log = 'Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss)\n",
    "                training_log += current_log + \"\\n\"\n",
    "                print(current_log)\n",
    "                \n",
    "                # Save every 100 epochs\n",
    "                if (epoch_i + 1) % save_every_n_epochs == 0:\n",
    "                    saver = tf.train.Saver()\n",
    "                    saver.save(sess, save_dir + '-' + run_id + '--c_epoch-' + str(epoch_i + 1))\n",
    "                    model_saved_msg = 'Model Trained and Saved - Epoch: ' + str(epoch_i + 1)\n",
    "                    print(model_saved_msg)\n",
    "                    training_log += model_saved_msg + \"\\n\"\n",
    "                \n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')\n",
    "    \n",
    "    text_file = open(logs_dir + \"training_log-{}.txt\".format(run_id), \"w\")\n",
    "    text_file.write(training_log)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new Vidyabagish text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "pickle.dump((seq_length, save_dir), open('params-{}.p'.format(run_id), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training results\n",
    "\n",
    "The table below captures the results of training the Vidyabagish Neural Network with different hyperparameters:\n",
    "\n",
    "| Run ID | Batch Size | Epochs | RNN Layers | RNN Size | Embed Dim | Seq Length | LR | Dropout | Train Loss |\n",
    "|:---:|:---:|:---:|:---:|:----:|:----:|:----:|:----:|:-----:|\n",
    "| 0001 | 512 | 300 | 2 | 256 | 300 | 5 | 0.01 | 0.6 | 3.438 |\n",
    "| 0002 | 512 | 500 | 2 | 256 | 300 | 5 | 0.001 | 0.6 | 1.488 |\n",
    "| 0003 | 512 | 300 | 2 | 256 | 300 | 10 | 0.01 | 0.6 | 3.015 |\n",
    "| 0004 | 512 | 500 | 2 | 256 | 300 | 10 | 0.001 | 0.6 | 1.112 |\n",
    "| 0005 | 512 | 500 | 3 | 256 | 300 | 10 | 0.001 | 0.6 | 1.178 |\n",
    "| 0006 | 512 | 500 | 3 | 256 | 300 | 20 | 0.001 | 0.6 | 1.317 |\n",
    "| 0007 | 512 | 700 | 2 | 256 | 300 | 10 | 0.001 | 0.6 | 0.998 |\n",
    "\n",
    "For a detailed view of the training loss, checkout the [training logs](./logs/) included with the project.\n",
    "\n",
    "Our models 0004 and 0007 are the best performing when trained with our dataset. \n",
    "\n",
    "How does our model performs when compared to our initial expectations (metrics/benchmark)?\n",
    "- Our benchmark was to create a model that achieves a perplexity of less than 3. Our model is achieving a perplexity of less than 1. Which means, it has a high likelihood of choosing the correct word.\n",
    "- The paragraph structure generated by our model is similar to the structure from our dataset.\n",
    "    - The sentences in the paragraph have betwen 10 and 15 words.\n",
    "    - The paragraphs are formed of 5 or more lines. \n",
    "\n",
    "*Observations:*\n",
    "- In order to generate better quality of text, we  need a large text corpus and in this project we are limited by the amount of text we can use as an input. Saying that, the model is generating text that looks similar to the original text.\n",
    "- The same algorithm can be used to generate text for different subjects. For example, the same algorithm can be trained to generate text for [Barack Obama](https://en.wikipedia.org/wiki/Barack_Obama). There is a lot more text available for Obama ([books](https://www.amazon.co.uk/Barack-Obama/e/B001H6OA8E), [speeches](http://obamaspeeches.com/))\n",
    "- A 3 layer RNN takes a long time to train, it might be possible to achieve better results with it but it will need to be trained longer.\n",
    "\n",
    "### Training Loss\n",
    "\n",
    "We can see that a learning rate of 0.01 is too large to train our Neural Network. When we trained it with 0.01, we were never able to achieve a train loss < 3. Another indicator of this is that the learning plateaus in both runs (0001, 0003); in *0001* it plateaus at around epoch 100 and in *0003* at around epoch 180.\n",
    "\n",
    "The training loss improved when we use a learning rate of 0.001. The lower learning rate improves our Neural network performance and we are getting closer to a perplexity of 1. Remember that the lower our perplexity, the better our model is at predicting the next work. \n",
    "\n",
    "As the training is not plateauing, we are also able to train it longer. This is why we increase the epochs of run *0002* to 500.\n",
    "\n",
    "### Sequence Length\n",
    "\n",
    "Our basic RNN was trained with a sequence length of 5. The sequence length, is the number of words to be included in every sequence.  \n",
    "\n",
    "We can see an improvement when increasing the sequence length to 10. This means that our RNN will use a longer sequence to train our Neural Network. Which ends up improving significantly the quality of text generated by our network. \n",
    "\n",
    "With a sequence length of 5, our text didn't made much sense, the sentences are short and the paragraphs are not well structured. \n",
    "\n",
    "When using a model trainer with sequence length of 10, we can notice that the text makes much more sense, and the quality of the sentences and paragraphs improves significantly. \n",
    "\n",
    "### Train some more\n",
    "\n",
    "Our best result in the the first 6 runs was run *0004*. If we train our network longer with the same parameters, we achieve a train loss of less than 1. \n",
    "\n",
    "--------------------\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Based on the results, we can see that RNNs are very effective when understanding sequence of text and can be used to generate text.\n",
    "\n",
    "Our best performing RNN consists of 2 RNN layers of 256 in size. We added some dropout to prevent overfitting and used [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) to optimise our model.\n",
    "\n",
    "Generating text is a hard problem to solve in Machine Learning. It takes  a long time to train a simple model and the larger the input the longer it will take to train the model properly. In our training, the models took close to an hour to train.\n",
    "\n",
    "Even though our train loss for the last run is less than 1, we can see in the samples below that the text generated by *run 0004* and *run 0007* models are similar in several ways, as you would expect when both models have a perplexity that is close to 1:\n",
    "- They are both able to open and close quotations\n",
    "- The text makes more sense when compared with *run 0001*, which is expected as the sequence length (10) used to train both models is longer than *run 0001* (5)\n",
    "- Paragraphs are well formed.\n",
    "- Sentence length is similar and close to the median sentence length (13).\n",
    "\n",
    "Text Samples:\n",
    "\n",
    "- Run 0004:\n",
    "    \" Senor,\" said Sancho,\" I mean to know from this perilous journey in the ugly which has been bound;\n",
    "    \" At any rate, Dulcinea,\" replied the actor\n",
    "- Run 0007:\n",
    "    Quixote or cost him his squire, unless indeed his wife might follow him\n",
    "    Don Quixote bade Sancho he settled three days with open his heart in fixing his affections should comply with Preciosa\n",
    "\n",
    "### Potential Improvements\n",
    "\n",
    "There are two ways which we could improve the results of our model:\n",
    "- Larger text corpus. We can search for more Vidyabagish novels available in English or we can train our model to learn Spanish, as it would be more probable to find the novels in Spanish than English.\n",
    "- Train our 3 layer, 20 sequence length RNN further. As our model is bigger, we could train it longer and potentially could have better results.\n",
    "\n",
    "--------------------\n",
    "\n",
    "Below, we will generate some text to check our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Vidyabagish Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before generating text, lets import our preprocessed data and the params of our run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, meta_dir = pickle.load(open('params-{}.p'.format(run_id), mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below will generate Vidyabagish text based on some input.\n",
    "- `load_dir`: Location where the graph metadata is saved\n",
    "- `prime_word`: First word used to generate text\n",
    "- `gen_length`: Length of text we want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # Adding randomness to the word returned\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]\n",
    "    #return int_to_vocab[np.argmax(probabilities)]\n",
    "\n",
    "def generate_text(load_dir, prime_word, gen_length):\n",
    "    \"\"\"\n",
    "    Generates text\n",
    "    :param load_dir: Location where the graph metadata is saved\n",
    "    :param prime_word: First word used to generate text\n",
    "    :param gen_length: How long the generated text will be\n",
    "    :return: Generated text\n",
    "    \"\"\"\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        input_text, initial_state, final_state, probs = Vidyabagishnn.get_tensors(loaded_graph)\n",
    "\n",
    "        # Sentences generation setup\n",
    "        gen_sentences = [prime_word]\n",
    "        prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "        # Generate sentences\n",
    "        for n in range(gen_length):\n",
    "            # Dynamic Input\n",
    "            dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "            dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "            # Get Prediction\n",
    "            probabilities, prev_state = sess.run(\n",
    "                [probs, final_state],\n",
    "                {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "            pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "            gen_sentences.append(pred_word)\n",
    "\n",
    "        # Remove tokens\n",
    "        generated_text = ' '.join(gen_sentences)\n",
    "        for key, token in token_dict.items():\n",
    "            ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "            generated_text = generated_text.replace(' ' + token.lower(), key)\n",
    "        generated_text = generated_text.replace('\\n ', '\\n')\n",
    "        generated_text = generated_text.replace('( ', '(')\n",
    "\n",
    "        return generated_text\n",
    "    \n",
    "def print_text_for(run_id, epochs, initial_word, initial_epoch=100, text_length=100):\n",
    "    for epoch in range(initial_epoch, epochs + 100, 100):\n",
    "        print('-----------\\n{} at run_id: {}, epoch: {}, text generated: \\n------------\\n{}'.format(initial_word, run_id, epoch, generate_text(meta_dir + '-' + run_id + '--c_epoch-' + str(epoch), initial_word, text_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start comparing text generated by our different Vidyabagish Neural Networks runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Quixote at run_id: 0001, epoch: 300, text generated: \n",
      "------------\n",
      "Quixote,\" and\n",
      "my profession abideth about her Majesty then, the cloth was frantic, that it\n",
      "is\n",
      "so hast been, that she has not spread of showing and tear resting with him, for the persuasion, leaving him by force or get up by full great\n",
      "achievements of what book,\n",
      "Leaves the pastime?\"\n",
      "\n",
      "\" I would lay myself upon him with\n",
      "a burnished hand to the waist.\n",
      "The thieves laughed at\n",
      "the other little intelligence, and where the redress, in the service of some dwarf, and\n",
      "we were in the line of\n",
      "mutual Sancho, this,\n",
      "I have not such matters of us with I not heard by which is the best world,\n",
      "Non when the bano seated myself and Sancho Panza asked the same of his\n",
      "story, they were excellent wife at once, and here the green intentions of the Judge so cautiously gave her. The blush, and heard them came to over a payment with shepherd, laying round it\n",
      "he ordered her, and\n"
     ]
    }
   ],
   "source": [
    "# Run 0001\n",
    "\n",
    "get_text_for(run_id='0001', epochs=300, initial_word=\"Quixote\", initial_epoch=300, text_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Quixote at run_id: 0004, epoch: 500, text generated: \n",
      "------------\n",
      "Quixote,\" and am the idea thou wilt give more\n",
      "good quickly to see now thou hast won the good\n",
      "thing, as I have told them not.\"\n",
      "\n",
      "\" Senor,\" said Sancho,\" I mean to know from\n",
      "this perilous journey in the ugly which has been bound; for it is\n",
      "that? What are it in me; but the knight-errant should come free?\"\n",
      "\n",
      "\" At any rate, Dulcinea,\" replied the actor held in nonsense of your wife, master the devil who is so generous that I can go to the house of\n",
      "Luscinda, Preciosa, and more will by everything the\n",
      "wrong the beauty itself ought to wash its course.\n",
      "\n",
      "\" What could mean be\" The greatest who is long in\n",
      "his own behalf so long as I have chosen to\n",
      "Uchali, who shall not go to defend your ass\n",
      "will run wrong, for the mercy I was now in a\n",
      "coach that, after having sold their riches and language that\n",
      "Preciosa asked\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_text_for(run_id='0004', epochs=500, initial_word=\"Quixote\", initial_epoch=500, text_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Quixote at run_id: 0007, epoch: 500, text generated: \n",
      "------------\n",
      "Quixote or cost him his squire, unless indeed his\n",
      "wife might follow him or with great respect. Their\n",
      "master would be, the blind parents describes, or\n",
      "look free, and all the rest with your life shall be imagined; the gipsy had\n",
      "rule come to his heart. The extreme day I have heard of her good; in\n",
      "the moment of this my house and soul to find\n",
      "the paper; though, as it were, we\n",
      "believe it is because they who are dead, and bind\n",
      "me to undergo the exertion thou great desire, for indeed he said to him cannot read it aloud.\n",
      "\n",
      "Don Quixote bade Sancho he settled three days with open his heart\n",
      "in fixing his affections should comply with Preciosa, and\n",
      "to keep secret whatever will be no wish on, as it was queens as his master had\n",
      "heard him, he strove to sing him, maintaining the sun favouring befall), which was not of it, both of them, served in her courtesy\n",
      "for the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_text_for(run_id='0007', epochs=500, initial_word=\"Quixote\", initial_epoch=500, text_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* NLP Tokenization - [https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\n",
    "\n",
    "* Vector Representations of Words - [https://www.tensorflow.org/tutorials/word2vec#motivation_why_learn_word_embeddings](https://www.tensorflow.org/tutorials/word2vec#motivation_why_learn_word_embeddings) \n",
    "\n",
    "* Recurrent Neural Networks - [https://www.tensorflow.org/tutorials/recurrent](https://www.tensorflow.org/tutorials/recurrent) \n",
    "\n",
    "* Alex Graves - Generating Sequences With Recurrent Neural Networks [https://arxiv.org/pdf/1308.0850.pdf](https://arxiv.org/pdf/1308.0850.pdf)\n",
    "\n",
    "* Christopher Olah - Understanding LSTM Networks [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) \n",
    "\n",
    "* Prasad Kawthekar, Raunaq Rewari, Suvrat Bhooshan - Evaluating Generative Models for Text Generation - [https://web.stanford.edu/class/cs224n/reports/2737434.pdf](https://web.stanford.edu/class/cs224n/reports/2737434.pdf) \n",
    "\n",
    "* Ilya Sutskever, James Martens, Geoffrey Hinton - Generating Text with Recurrent Neural Networks - [http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf](http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
